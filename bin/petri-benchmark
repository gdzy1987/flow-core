#!/usr/bin/env python

from flow.orchestrator.client import OrchestratorClient
import argparse
import flow.brokers.amqp
import flow.orchestrator.redisom as rom
import flow.petri.net as pn
import json
import os
import pika
import redis
import subprocess
import sys
import time

def shellcmd_net():
    net = pn.SuccessFailurePetriNet.create(connection=conn, name="S/F Shell Command")
    running = net.add_place(name="running")
    msg_exec_ok = net.add_place(name="msg_exec_ok")
    msg_exec_fail = net.add_place(name="msg_exec_fail")

    # "execute" transition action
    execute_cmd = pn.ShellCommand.create(connection=conn, cmdline=["ls", "-alrt"],
            success_place_key=msg_exec_ok.key,
            failure_place_key=msg_exec_fail.key)

    # Create transitions
    execute = net.add_transition(name="execute", actions=[execute_cmd.key])
    exec_success = net.add_transition(name="exec_success")
    exec_failure = net.add_transition(name="exec_failure")

    # Add arcs to hook everything up
    net.add_arc(net.start, execute)
    net.add_arc(execute, running)
    net.add_arc(running, exec_success)
    net.add_arc(running, exec_failure)
    net.add_arc(msg_exec_ok, exec_success)
    net.add_arc(msg_exec_fail, exec_failure)
    net.add_arc(exec_success, net.success)
    net.add_arc(exec_failure, net.failure)
    return net


def build_net(num_places):
    net = pn.SuccessFailurePetriNet.create(connection=conn, name="S/F Shell Command")
    begin = net.add_transition(name="Begin")
    end = net.add_transition(name="End")
    net.add_arc(net.start, begin)
    net.add_arc(end, net.success)

    for x in xrange(num_places):
        place = net.add_place(name="Place %d" % x)
        net.add_arc(begin, place)
        net.add_arc(place, end)

    return net


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--num-flows", default=1, type=int,
                        help="The total number of flows to construct")
    parser.add_argument("--nodes-each", default=100, type=int,
                        help="The number of nodes in each flow")
    parser.add_argument("--sleep-time", default=0, type=float,
                        help="Time in seconds each node should sleep")
    parser.add_argument("--amqp-url", default=None,
                        help="If specified, launch the created flows by "
                             "sending a message on the givel url")
    redis_url = parser.add_mutually_exclusive_group(required=True)
    redis_url.add_argument("--redis-unix",
                           help="Path to redis UNIX domain socket")
    redis_url.add_argument("--redis-url", help="host[:port] of redis server")
    args = parser.parse_args()

    if args.redis_unix:
        conn = redis.Redis(unix_socket_path=args.redis_unix)
    else:
        if ":" in args.redis_url:
            host, port = args.redis_url.split(":")
            conn = redis.Redis(host=host, port=int(port))
        else:
            host = args.redis_url
            conn = redis.Redis(host=host)

    print "num_flows=%d" % args.num_flows
    print "nodes_each=%d" % args.nodes_each
    print "sleep_time=%f" % args.sleep_time

    beg = time.time()
    net = build_net(args.nodes_each)
    end = time.time()
    print "net=%s" % str(net.key)
    print "construct_sec=%f" % (end - beg)

    if args.amqp_url:
        #os.environ["AMQP_URL"] = args.amqp_url
        #broker = flow.brokers.amqp.AmqpBroker()
        #orchestrator_service = OrchestratorClient(broker,
                #execute_node_routing_key='flow.node.execute')

        #orchestrator_service.execute_node(str(net.key))
        #broker.listen()

        routing_key = "flow.node.execute"
        body = json.dumps({
            "node_key": str(net.start.key),
            "num_tokens": 1,
            "message_class": "AddTokensMessage",
        })

        conn = pika.BlockingConnection(pika.URLParameters(args.amqp_url))
        qchannel = conn.channel()
        qchannel.exchange_declare(
            exchange="workflow",
            exchange_type="topic",
            durable=True,
            arguments={"alternate-exchange": "workflow.alt"}
            )
        qchannel.basic_publish(
            exchange="workflow",
            routing_key=routing_key,
            body=body,
            properties=pika.BasicProperties(
                delivery_mode=2,
            )
        )

        blocker = ["wait_for_flow.py", str(net.key)]
        null = open("/dev/null", "w")
        exit_code = subprocess.call(blocker, stdout=null, stderr=null)
        if exit_code != 0:
            print "Flow execution failed!"
        else:
            print "Yay"
            print net.status
            print "execute_sec: %f seconds" % net.duration
        sys.exit(exit_code)
