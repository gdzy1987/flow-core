#!/usr/bin/env python

from flow.orchestrator.client import OrchestratorClient
import argparse
import flow.brokers.amqp
import flow.orchestrator.redisom as rom
import flow.orchestrator.types as nodes
import json
import os
import pika
import redis
import subprocess
import sys
import time


def build_flow(conn, size, sleep_time=None, **kwargs):
    flow = nodes.Flow.create(connection=conn, **kwargs)
    start_node = nodes.StartNode.create(connection=conn, flow_key=flow.key,
                                        name="start node")
    stop_node = nodes.StopNode.create(connection=conn,
                                      flow_key=flow.key,
                                      indegree=size,
                                      name="stop node")

    node_indices = []
    node_keys = [start_node.key, stop_node.key]
    for i in xrange(size):
        name = "Node %d" % i
        node = nodes.SleepNode.create(
                connection=conn, name=name,
                flow_key=flow.key,
                indegree=1,
                sleep_time=sleep_time,
                successors=set([1]),
                )
        index = len(node_keys)
        node_indices.append(index)
        node_keys.append(node.key)

    flow.node_keys.value = node_keys
    start_node.successors.value = node_indices

    return flow


def build_all_flows(conn, args):
    master_flow = nodes.Flow.create(connection=conn, name="Benchmark flow")
    start_node = nodes.StartNode.create(connection=conn, name="start")
    stop_node = nodes.StopNode.create(connection=conn, name="stop",
                                      indegree=args.num_flows)
    child_flows = []
    child_flow_successors = set([1]) # stop node is index 1

    first_idx = master_flow.add_nodes([start_node, stop_node])
    for i in xrange(args.num_flows):
        child_flow = build_flow(conn, args.nodes_each,
                                sleep_time=args.sleep_time,
                                name="Test Workflow %d" % i)
        child_flow.indegree = 1
        child_flow.successors = child_flow_successors
        child_flows.append(child_flow)
    master_flow.add_nodes(child_flows)
    start_node.successors = set([x+first_idx for x in xrange(args.num_flows)])

    return master_flow


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--num-flows", default=1, type=int,
                        help="The total number of flows to construct")
    parser.add_argument("--nodes-each", default=100, type=int,
                        help="The number of nodes in each flow")
    parser.add_argument("--sleep-time", default=0, type=float,
                        help="Time in seconds each node should sleep")
    parser.add_argument("--amqp-url", default=None,
                        help="If specified, launch the created flows by "
                             "sending a message on the givel url")
    redis_url = parser.add_mutually_exclusive_group(required=True)
    redis_url.add_argument("--redis-unix",
                           help="Path to redis UNIX domain socket")
    redis_url.add_argument("--redis-url", help="host[:port] of redis server")
    args = parser.parse_args()

    if args.redis_unix:
        conn = redis.Redis(unix_socket_path=args.redis_unix)
    else:
        if ":" in args.redis_url:
            host, port = args.redis_url.split(":")
            conn = redis.Redis(host=host, port=int(port))
        else:
            host = args.redis_url
            conn = redis.Redis(host=host)

    print "num_flows=%d" % args.num_flows
    print "nodes_each=%d" % args.nodes_each
    print "sleep_time=%f" % args.sleep_time

    beg = time.time()
    master_flow = build_all_flows(conn, args)
    end = time.time()
    print "master_flow=%s" % str(master_flow.key)
    print "construct_sec=%f" % (end - beg)

    if args.amqp_url:
        #os.environ["AMQP_URL"] = args.amqp_url
        #broker = flow.brokers.amqp.AmqpBroker()
        #orchestrator_service = OrchestratorClient(broker,
                #execute_node_routing_key='flow.node.execute')

        #orchestrator_service.execute_node(str(master_flow.key))
        #broker.listen()

        routing_key = "flow.node.execute"
        body = json.dumps({
            "node_key": master_flow.key,
            "message_class": "ExecuteNodeMessage",
        })

        conn = pika.BlockingConnection(pika.URLParameters(args.amqp_url))
        qchannel = conn.channel()
        qchannel.exchange_declare(
            exchange="workflow",
            exchange_type="topic",
            durable=True,
            arguments={"alternate-exchange": "workflow.alt"}
            )
        qchannel.basic_publish(
            exchange="workflow",
            routing_key=routing_key,
            body=body,
            properties=pika.BasicProperties(
                delivery_mode=2,
            )
        )

        blocker = ["wait_for_flow.py", str(master_flow.key)]
        exit_code = subprocess.call(blocker)
        status = str(master_flow.status)
        if exit_code != 0 or status != nodes.Status.success:
            print "Flow execution failed!"
        else:
            print "execute_sec: %f seconds" % master_flow.duration
        sys.exit(exit_code)
